%-------------------------------------------------------------------------------
\subsection{Bidirectional UDP 1 Gbit/s}
This section presents a set of performance and efficiency tests performed on the Device Under Test (DUT) using bidirectional UDP traffic at a total rate of 1\,Gbit/s. 
The goal is to evaluate the forwarding performance of the DUT under varying packet sizes, simulating realistic traffic patterns with increasing stress on the packet-processing path.

To provide a comprehensive and representative view, the tests are structured into five subsections, each corresponding to a different Ethernet frame size. 
Four of the selected sizes -- 64 bytes, 512 bytes, 1280 bytes, and 1518 -- bytes are recommended by RFC2544\cite{rfc2544} 
covering both edge-cases and practically relevant intermediate values. 
The fifth size, 888 bytes, was chosen becuase it was identified as the average size in real-world network traffic by Jurkiewicz et al.~\cite{JURKIEWICZ202115}. 
This selection covers the full range of standard Ethernet frame sizes, from the minimum to the maximum non-jumbo frames, 
and includes a representative average frame size observed in real network traffic.

Traffic in all scenarios is generated using TRex with the \textit{udp\_1pkt\_src\_ip\_split.py} profile, 
which ensures that each packet carries a unique source IP address to simulate multiple concurrent clients, while maintaining a single destination IP per direction. 
The routing table of the DUT contains only two active forwarding entries, corresponding to the test routes, in addition to two administrative entries used for management.
The total offered load is 1\,Gbit/s, symmetrically split between both directions (500\,Mbit/s each).
The chosen load of 1 Gbit/s is representative of a realistic aggregate traffic pattern that could be observed in a small or medium-sized enterprise network, especially when routed through a central gateway.

The DUT is configured with the Vector Packet Processing (VPP) stack and tested under three levels of parallelism: using 1, 4, and 10 worker threads. 
The number of RX/TX queues is aligned with the number of active worker threads in each configuration to ensure balanced packet distribution and optimal resource utilization.

To provide a baseline for comparison, all scenarios are also executed using the standard Linux kernel networking stack, 
configured with equivalent routing and interface parameters, but able to use all 20 cores of CPU. 
This allows for a direct comparison between VPP and traditional kernel-based forwarding in terms of performance and energy efficiency.

The aim of this test is to observe the behavior of the VPP forwarding plane under low traffic load of small packets and to evaluate its energy efficiency.

%-------------------------------------------------------------------------------
\subsubsection{64-bytes frames}
This test evaluates the behavior of the VPP forwarding plane under a low-throughput traffic load composed of minimum-sized Ethernet frames (64 bytes). 
Such frames result in a high packet-per-second rate for a given bandwidth, which increases the processing overhead per bit of data. 
This configuration represents a stress scenario for packet forwarding and allows assessment of the system’s efficiency in handling a large number of small packets.

\begin{table}[h!]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{Configuration}} &
\multicolumn{1}{c|}{\textbf{Watts used}} &
\multicolumn{1}{c|}{\textbf{PPW}} &
\multicolumn{1}{c|}{\textbf{BPW}} \\
\hline
VPP -- 1 worker & 848.08 & 20 726 965.60 & 1 326 525 798.50 \\
VPP -- 4 workers & 951.03 & 18 483 249.76 & 1 182 927 982.40 \\
VPP -- 10 workers & 1 193.56 & 14 727 474.94 & 942 558 396.09 \\
Linux stack & 1 257.25 & 13 981 407.81 & 894 810 100.29 \\
\hline
\end{tabular}
\caption{Result of Bidirectional UDP 1 Gbit/s of 64-bytes packets test}
\label{tab:udp:one}
\end{table}

As the results in Table \ref{tab:udp:one} show, the power consumption increases notably with the number of worker threads in the VPP stack. 
While all VPP configurations deliver identical packet and byte throughput, the most energy-efficient setup is this measurement is the single-worker variant, consuming roughly 850 Watts during the test. 
In contrast, the traditional Linux network stack demonstrates the highest energy usage, despite handling the same volume of packets.

This discrepancy can likely be attributed to the cost of processing a high number of small packets in kernel space. 
Since the test uses fixed-size 64-byte packets, which are known to generate frequent system calls and context switches in Linux, 
the forwarding path becomes less efficient compared to VPP’s user-space architecture, where such overheads are significantly reduced. 
The results highlight the energy cost of kernel-based packet forwarding in scenarios dominated by small-packet traffic.

%------------------------------------------------------------------------------
\subsubsection{512-bytes frames}
TODO - CITOVAT DNS OD CZ NIC NA VELIKOST DNS
This test focuses on the forwarding performance of the VPP data plane when processing medium-sized Ethernet frames of 512 bytes. 
These frames offer a balance between protocol overhead and payload efficiency and are representative of many real-world applications that do not utilize maximum frame sizes. 
The test helps to evaluate how the system handles typical traffic patterns with moderate packet rates and processing demands.

\begin{table}[h!]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{Configuration}} &
\multicolumn{1}{c|}{\textbf{Watts used}} &
\multicolumn{1}{c|}{\textbf{PPW}} &
\multicolumn{1}{c|}{\textbf{BPW}} \\
\hline
VPP -- 1 worker & 851.21 & 2 581 343.78 & 1 321 648 015.98 \\
VPP -- 4 workers & 969.49 & 2 266 413.93 & 1 160 403 931.63 \\
VPP -- 10 workers & 1 175.07 & 1 869 901.91 & 957 389 779.06 \\
Linux stack &  &  &  \\
\hline
\end{tabular}
\caption{Result of Bidirectional UDP 1 Gbit/s of 512-bytes frames test}
\label{tab:udp:one}
\end{table}

%------------------------------------------------------------------------------
\subsubsection{TODO 888-bytes frames}

This test focuses on the forwarding performance of the VPP data plane when processing medium-sized Ethernet frames of 888 bytes. 
These frames offer a balance between protocol overhead and payload efficiency. As said above this size was found as the average size in real-work network traffic.

The test helps to evaluate how the system handles traffic patterns with moderate packet rates and processing demands, 

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Configuration} & \textbf{Transmited packets} & \textbf{Transmited bytes} & \textbf{Watts used} \\
\hline
VPP -- 1 worker & 1 293 103 500 & 1 125 000 045 000 & 823.20\\
VPP -- 4 workers & 1 293 103 500 & 1 125 000 045 000 & 972.49\\
VPP -- 10 workers & 1 293 103 500 & 1 125 000 045 000 & 1 170.26\\
Linux stack & 1 293 103 500 & 1 125 000 045 000 & 946.28\\
\hline
\end{tabular}
\caption{Result of Bidirectional UDP 1 Gbit/s of 870-bytes packets test}
\label{tab:udp:three}
\end{table}

As shown in table \ref{tab:udp:two}, the second test with 870-byte packets, 
VPP's power consumption remains relatively stable across different worker thread configurations, with a variation of only about ±5\% compared to previous measurement. 
This indicates that VPP's performance and power consumption are consistent, regardless of the increased packet size. 
On the other hand, the Linux stack's power consumption shows a positive reduction compared to the previous test, which is attributed to the lower number of packets being processed. 
Since larger packets reduce the packet-per-second (PPS) rate, the system overhead and processing cost in Linux are lower, leading to more efficient energy usage in this scenario.

%-------------------------------------------------------------------------------
\subsubsection{1280-bytes frames}
This test focuses on the forwarding performance of the VPP data plane when processing large Ethernet frames of 1280 bytes. 
These frames represent a size commonly used in environments where larger payloads are transferred, but without exceeding the typical Ethernet MTU limit of 1500 bytes. 
This size strikes a balance between higher data throughput and maintaining efficient protocol overhead.

The test evaluates how the system handles traffic patterns with higher packet sizes, which are common in applications involving large data transfers such as video streaming, 
file sharing, or other high-bandwidth scenarios in enterprise and data center environments.

\begin{table}[h!]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{Configuration}} &
\multicolumn{1}{c|}{\textbf{Watts used}} &
\multicolumn{1}{c|}{\textbf{PPW}} &
\multicolumn{1}{c|}{\textbf{BPW}} \\
\hline
VPP -- 1 worker & 866.91 & 1 013 837.98 & 1 297 712 609.61 \\
VPP -- 4 workers & 961.01 & 914 565.18 & 1 170 643 425.56 \\
VPP -- 10 workers & 1188.39 & 739 577.31 & 946 658 957.41 \\
Linux stack & 926.77 & 948 354.26 & 1 213 893 456.20 \\
\hline
\end{tabular}
\caption{Result of Bidirectional UDP 1 Gbit/s of 1280-frames test}
\label{tab:udp:five}
\end{table}



%-------------------------------------------------------------------------------
\subsubsection{1518-bytes frames}
This test evaluates the performance of the VPP forwarding plane when handling full-sized Ethernet frames (1518 bytes, including headers). 
These frames represent the upper limit of standard Ethernet without jumbo frame extensions and represent throughput efficiency under optimal conditions. 
Meaning that the ratio of payload to protocol overhead is maximized. The goal is to observe how the system behaves when forwarding large packets that minimize per-packet processing overhead.

\begin{table}[h!]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\multicolumn{1}{|c|}{\textbf{Configuration}} &
\multicolumn{1}{c|}{\textbf{Watts used}} &
\multicolumn{1}{c|}{\textbf{PPW}} &
\multicolumn{1}{c|}{\textbf{BPW}} \\
\hline
VPP -- 1 worker & 854.66 & 867 136.33 & 1 316 312 956.40 \\
VPP -- 4 workers & 972.93 & 761 726.68 & 1 156 301 102.16 \\
VPP -- 10 workers & 1 192.34 & 621 556.55 & 943 522 846.94 \\
Linux stack & 934.24 & 793 272.33 & 1 204 187 394.37 \\
\hline
\end{tabular}
\caption{Result of Bidirectional UDP 1 Gbit/s of 1518-frames test}
\label{tab:udp:five}
\end{table}

As shown in Table \ref{tab:udp:five}, even when using the maximum standard Ethernet frame size -- thus minimizing the number of interrupts and per-packet processing overhead -- 
Linux does not outperform VPP running on single core (with one worker-thread) in terms of energy efficiency.

This shows that the Linux networking stack must perform a number of energy-intensive operations, 
which cannot be fully offset even under ideal conditions with large frame sizes and reduced interrupt overhead. 
As a result, it remains less efficient than VPP running on a single core.

\subsubsection{Test Conclusion}





