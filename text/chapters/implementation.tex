VPP’s dataplane is implemented by four main architectural layers: VPPINFRA, VNET, VLIB, and Plugins. 
Each of these layers provides distinct functionality that supports efficient networking operations, from low-level data structure management to high-level network function optimizations.

VPPINFRA provides foundational libraries for tasks such as memory handling, vectors, rings, hash table lookups, and timers. 
VNET focuses on implementing network protocols for layers 2 to 4 and includes the control plane. 
VLIB serves as the runtime environment for vectorized processing and also provides the command-line interface. 
Finally, plugins allow the system to be extended or customized by adding new features or modifying existing ones~\cite{fdio-vpp-softwarearchitecture-2506}.

%--------------------------------------------------------
\subsection{VPPINFRA}
VPPINFRA is a collection of foundational libraries designed to provide high-performance capabilities for various internal tasks within VPP. 
It includes dynamic arrays, hash tables, bitmaps, timing utilities, logging mechanisms, and data structure serialization, all optimized for speed and efficiency.~\cite{fdio-vpp-infrastructure-2506}

\begin{itemize}
  \item \textbf{Vectors} –- dynamically resized arrays with user-defined headers. 
Vectors are used as the basis for other structures such as pools or hash tables and support efficient memory reuse through safe length resetting.~\cite{fdio-vpp-infrastructure-2506}

  \item \textbf{Bitmaps} -–  compact data structures used to efficiently track the true/false state of multiple indexed items using individual bits, built on top of vectors.~\cite{fdio-vpp-infrastructure-2506}

  \item \textbf{Pools} –- structures used to quickly allocate and free fixed-size data structures, such as packet buffers or per-session metadata. 
Internally, they are implemented using vectors and bitmaps.~\cite{fdio-vpp-infrastructure-2506}

  \item \textbf{Hashes} -- lookup structures optimized for fast access using hash functions.~\cite{fdio-vpp-infrastructure-2506}

  \item \textbf{Timekeeping} -- utilities providing precise, low-overhead timing based on CPU cycles. 
VPPINFRA continuously adjusts its time calibration by comparing CPU ticks against kernel time, ensuring accurate time measurement without expensive system calls.~\cite{fdio-vpp-infrastructure-2506}

  \item \textbf{Timer wheel} -- subsystem for efficiently managing timers and periodic events. 
It supports multiple configuration options, including the number of wheels, slots, and timers per object, 
allowing high-performance scheduling in time-sensitive applications.~\cite{fdio-vpp-infrastructure-2506}

  \item \textbf{Logging and formatting} -- includes support for fast event logging, trace output, and data formatting used for debugging and diagnostics.~\cite{fdio-vpp-infrastructure-2506}

  \item \textbf{Serialization} -- support for serializing and deserializing internal data structures for persistent storage or communication between threads.~\cite{fdio-vpp-infrastructure-2506}
\end{itemize}

%--------------------------------------------------------
\subsection{VNET}
VNET (VPP Network Stack) implements the core networking logic in VPP, providing graph nodes for Layer 2 and Layer 3 packet processing.

A key mechanism provided by VNET is the concept of feature arcs. 
These represent named sequences of graph nodes within the packet processing graph, allowing custom nodes -- such as NAT, ACLs, or telemetry -- to be inserted into existing pipelines in a defined order. 
Feature arcs enable modular composition of processing features without modifying the core graph logic. For example, an ACL node can be inserted at the beginning of the 
\texttt{ip4-unicast}\footnote{\texttt{ip4-unicast} is a feature arc that processes unicast IPv4 packets before they reach the routing logic.} feature arc.

In addition to protocol and interface handling, VNET also provides a flexible framework for packet tracing, 
allowing developers to inspect and debug the path that each packet takes through the graph in fine detail. 
This is especially useful for analyzing the behavior of custom nodes or diagnosing complex feature interactions.

Finally, VNET includes a built-in packet generator, which can be used to simulate traffic and evaluate the performance of specific graph paths under 
controlled conditions.~\cite{fdio-vpp-vnet-2506, fdio-vpp-featurearcs-2506}

%--------------------------------------------------------
\subsection{VLIB}
VLIB provides the runtime environment and execution engine that powers VPP’s packet processing model.
One of its core responsibilities is managing the registration and execution of all graph nodes, including input, internal and process nodes.

The execution of graph nodes in VPP is coordinated by a lightweight cooperative scheduler.
Each iteration of the main loop begins with input nodes producing vectors of packets, which are then passed through a sequence of internal nodes forming a directed graph.
Nodes process the incoming vector and determine, for each packet, which next node should process it based on routing, classification, or protocol-specific logic.

Packets destined for the same next node are grouped together and placed into a new \texttt{vlib\_frame\_t}, which is then enqueued to that node for processing.
This selective forwarding enables efficient vector splitting and maintains high performance by improving cache locality and reducing per-packet overhead.

When a node cannot or should not be executed immediately, 
VPP defers its execution by adding it to a list of pending operations using \texttt{pending\_frames}.
These frames are processed later in the main loop.

VLIB also provides the command-line interface (CLI), which allows operators to interact with the VPP runtime.~\cite{fdio-vpp-vlib-2506}

%--------------------------------------------------------
\subsection{Plugins}
Plugins in VPP are implemented as shared object libraries that are automatically discovered and loaded by VLIB during startup. 
They allow developers to add new features or extend existing functionality without modifying the VPP core.

To create a plugin, developers add a new directory under \texttt{src/plugins}, define build instructions using \texttt{plugin.mk} and \texttt{CMakeLists.txt}, and implement the required logic. 
After compilation, the plugin is placed into the designated plugin directory and becomes available for VPP to load.

This modular architecture enables rapid experimentation and integration of custom network functions while keeping the base system clean and maintainable.~\cite{fdio-vpp-plugins-2506, fdio-vpp-addplugin-2506}

%--------------------------------------
\subsection{Installation, Configuration and Startup}
VPP officially supports the \textit{x86\_64} and \textit{ARM-AArch64} architectures and can be installed on recent LTS versions of Debian and Ubuntu Linux distributions.~\cite{fdio-supported-2025}
Prebuilt packages for these systems are provided via the official FD.io package repository hosted on Packagecloud.io~\cite{fdio-packagecloud}.
Alternatively, VPP can be built from source code to enable custom builds, which can be useful for development purposes.
This approach also enables installation on distributions such as Red Hat and CentOS, which are not officially supported by the prebuilt packages.~\cite{fdio-building-guide}.

\subsubsection{System Preparation}
After installation, VPP registers itself as a system service, and a corresponding systemd unit file is created at \texttt{/usr/lib/systemd/system/vpp.service}. 
This enables standard service management commands such as \texttt{systemctl start vpp} or \texttt{systemctl status vpp} to control the daemon. 
The service uses the default configuration defined in \texttt{/etc/vpp/startup.conf}, unless otherwise specified.

Since VPP requires hugepages, the installation process places a configuration file at \texttt{/etc/sysctl.d/80-vpp.conf} to override system defaults. 
By default, this sets the number of 2~MB hugepages to~1024.\footnote{This is a system-wide setting, not specific to VPP.}~\cite{fdio-running}

Before running VPP with DPDK, the relevant network interfaces must either be detached from their kernel drivers and bound to a kernel module providing user-space access, such as \texttt{vfio-pci},
or used with a bifurcated driver\footnote{In this case, the NIC remains under kernel control, while the data path is handled by the PMD.}.
In the former case, the \texttt{dpdk-devbind.py} utility can be used to rebind interfaces as needed.
This setup is required for DPDK to successfully initialize the specified network interface.~\cite{dpdk-linux-drivers}

\subsubsection{Startup Configuration}
Although VPP can be started with command-line arguments, such as \verb|/usr/bin/vpp unix { interactive cli-listen 127.0.0.1:5002 }|, which can be useful for debugging purposes, 
it is typically started using a configuration file. 
The default configuration file is located at \texttt{/etc/vpp/startup.conf}, but a custom file can also be specified.
When not using \texttt{systemd}, a specific configuration file can be passed manually via the \texttt{-c} parameter, 
for example: \verb|/usr/bin/vpp -c /etc/vpp/startup.conf|.~\cite{fdio-config-getting-started} 

The configuration file is divided into several sections. The following overview summarizes the most relevant ones, as described in the official configuration reference.~\cite{fdio-config-reference}

\begin{itemize}
  \item \textbf{The unix section} defines the general runtime behavior of the VPP process. 
It includes parameters that control how the application runs, where it logs output, and how the command-line interface is exposed. 
The \texttt{nodaemon} parameter prevents VPP from running in the background. 
The \texttt{log} parameter specifies the path to the log file, typically set to \texttt{/var/log/vpp/vpp.log}. 
The \texttt{cli-listen} parameter defines how the VPP CLI is made accessible -- by default through a UNIX domain socket at \texttt{/run/vpp/cli.sock}. 
Additionally, the \texttt{startup-config} parameter (or its alias \texttt{exec}) can be used to provide a file containing CLI commands that are executed automatically after VPP starts.

  \item \textbf{The api-trace section} controls how API message tracing is handled within VPP.  
This section is primarily used for debugging and development purposes. 
API message tracing is typically enabled using the \texttt{api-trace on} command.

  \item \textbf{The cpu section} is used to configure how VPP threads are mapped to CPU cores.  
The \texttt{main-core} parameter specifies the core on which the main thread runs.  
The \texttt{corelist-workers} parameter defines a list of cores used by worker threads responsible for packet processing.  
The \texttt{scheduler-policy} parameter allows selecting a thread scheduling strategy, such as \texttt{fifo}, \texttt{rr}, or others, depending on the desired behavior.

  \item \textbf{The buffers section} allows configuring the size and number of memory buffers used by VPP for packet processing.  

  \item \textbf{The dpdk section} configures how DPDK is initialized and used by VPP.
The \texttt{dev} parameter specifies the PCI address of a network interface to be managed by VPP/DPDK.  
The \texttt{uio-driver} parameter sets the kernel module providing user-space access to be used, commonly \texttt{vfio-pci}.  
The \texttt{num-mbufs} parameter defines the number of memory buffers allocated for packet processing.  
Additional options include parameters such as \texttt{num-rx-queues} and \texttt{num-tx-queues}, which specify the number of receive and transmit queues for each device.

  \item \textbf{The plugins section} allows enabling or disabling individual VPP plugins during startup.  
Each plugin can be explicitly marked as \texttt{enable} or \texttt{disable}, depending on whether it should be loaded. It is also possible to disable all plugins by default.  

\end{itemize}

Other configuration sections exist for advanced or highly specific use cases, such as memory tuning, internal event logging, or telemetry export\dots; 
and are, to the author's best knowledge, used only in fine-tuned or specialized deployments.












